{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD0Prediction_1.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-dCraCmYEIyB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "984e33b5-fe5f-4626-9ba3-04027918ab8d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530754730072,
          "user_tz": 360,
          "elapsed": 165,
          "user": {
            "displayName": "Kay Park",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115858547447612857691"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Grid: # Environment\n",
        "  def __init__(self, width, height, start):\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "    self.i = start[0]\n",
        "    self.j = start[1]\n",
        "\n",
        "  def set(self, rewards, actions):\n",
        "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
        "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
        "    self.rewards = rewards\n",
        "    self.actions = actions\n",
        "\n",
        "  def set_state(self, s):\n",
        "    self.i = s[0]\n",
        "    self.j = s[1]\n",
        "\n",
        "  def current_state(self):\n",
        "    return (self.i, self.j)\n",
        "\n",
        "  def is_terminal(self, s):\n",
        "    return s not in self.actions\n",
        "\n",
        "  def move(self, action):\n",
        "    # check if legal move first\n",
        "    if action in self.actions[(self.i, self.j)]:\n",
        "      if action == 'U':\n",
        "        self.i -= 1\n",
        "      elif action == 'D':\n",
        "        self.i += 1\n",
        "      elif action == 'R':\n",
        "        self.j += 1\n",
        "      elif action == 'L':\n",
        "        self.j -= 1\n",
        "    # return a reward (if any)\n",
        "    return self.rewards.get((self.i, self.j), 0)\n",
        "\n",
        "  def undo_move(self, action):\n",
        "    # these are the opposite of what U/D/L/R should normally do\n",
        "    if action == 'U':\n",
        "      self.i += 1\n",
        "    elif action == 'D':\n",
        "      self.i -= 1\n",
        "    elif action == 'R':\n",
        "      self.j -= 1\n",
        "    elif action == 'L':\n",
        "      self.j += 1\n",
        "    # raise an exception if we arrive somewhere we shouldn't be\n",
        "    # should never happen\n",
        "    assert(self.current_state() in self.all_states())\n",
        "\n",
        "  def game_over(self):\n",
        "    # returns true if game is over, else false\n",
        "    # true if we are in a state where no actions are possible\n",
        "    return (self.i, self.j) not in self.actions\n",
        "\n",
        "  def all_states(self):\n",
        "    # possibly buggy but simple way to get all states\n",
        "    # either a position that has possible next actions\n",
        "    # or a position that yields a reward\n",
        "    return set(self.actions.keys()) | set(self.rewards.keys())\n",
        "\n",
        "\n",
        "def standard_grid():\n",
        "  # define a grid that describes the reward for arriving at each state\n",
        "  # and possible actions at each state\n",
        "  # the grid looks like this\n",
        "  # x means you can't go there\n",
        "  # s means start position\n",
        "  # number means reward at that state\n",
        "  # .  .  .  1\n",
        "  # .  x  . -1\n",
        "  # s  .  .  .\n",
        "  g = Grid(3, 4, (2, 0))\n",
        "  rewards = {(0, 3): 1, (1, 3): -1}\n",
        "  actions = {\n",
        "    (0, 0): ('D', 'R'),\n",
        "    (0, 1): ('L', 'R'),\n",
        "    (0, 2): ('L', 'D', 'R'),\n",
        "    (1, 0): ('U', 'D'),\n",
        "    (1, 2): ('U', 'D', 'R'),\n",
        "    (2, 0): ('U', 'R'),\n",
        "    (2, 1): ('L', 'R'),\n",
        "    (2, 2): ('L', 'R', 'U'),\n",
        "    (2, 3): ('L', 'U'),\n",
        "  }\n",
        "  g.set(rewards, actions)\n",
        "  return g\n",
        "\n",
        "\n",
        "def negative_grid(step_cost=-0.1):\n",
        "  # in this game we want to try to minimize the number of moves\n",
        "  # so we will penalize every move\n",
        "  g = standard_grid()\n",
        "  g.rewards.update({\n",
        "    (0, 0): step_cost,\n",
        "    (0, 1): step_cost,\n",
        "    (0, 2): step_cost,\n",
        "    (1, 0): step_cost,\n",
        "    (1, 2): step_cost,\n",
        "    (2, 0): step_cost,\n",
        "    (2, 1): step_cost,\n",
        "    (2, 2): step_cost,\n",
        "    (2, 3): step_cost,\n",
        "  })\n",
        "  return g\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wE2C3UkjD_0f",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "6ce3d776-2d1c-459f-c92a-10e11e5af561",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530754733642,
          "user_tz": 360,
          "elapsed": 280,
          "user": {
            "displayName": "Kay Park",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115858547447612857691"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "SMALL_ENOUGH = 1e-3\n",
        "GAMMA = 0.9\n",
        "ALPHA = 0.1\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "# NOTE: this is only policy evaluation, not optimization\n",
        "\n",
        "def random_action(a, eps=0.1):\n",
        "  # we'll use epsilon-soft to ensure all states are visited\n",
        "  # what happens if you don't do this? i.e. eps=0\n",
        "  p = np.random.random()\n",
        "  if p < (1 - eps):\n",
        "    return a\n",
        "  else:\n",
        "    return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding rewards (not returns as in MC)\n",
        "  # start at the designated start state\n",
        "  s = (2, 0)\n",
        "  grid.set_state(s)\n",
        "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
        "  while not grid.game_over():\n",
        "    a = policy[s]\n",
        "    a = random_action(a)\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    states_and_rewards.append((s, r))\n",
        "  return states_and_rewards\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  grid = standard_grid()\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'R',\n",
        "    (2, 3): 'U',\n",
        "  }\n",
        "\n",
        "  # initialize V(s) and returns\n",
        "  V = {}\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "\n",
        "  # repeat until convergence\n",
        "  for it in range(1000):\n",
        "\n",
        "    # generate an episode using pi\n",
        "    states_and_rewards = play_game(grid, policy)\n",
        "    # the first (s, r) tuple is the state we start in and 0\n",
        "    # (since we don't get a reward) for simply starting the game\n",
        "    # the last (s, r) tuple is the terminal state and the final reward\n",
        "    # the value for the terminal state is by definition 0, so we don't\n",
        "    # care about updating it.\n",
        "    for t in range(len(states_and_rewards) - 1):\n",
        "      s, _ = states_and_rewards[t]\n",
        "      s2, r = states_and_rewards[t+1]\n",
        "      # we will update V(s) AS we experience the episode\n",
        "      V[s] = V[s] + ALPHA*(r + GAMMA*V[s2] - V[s])\n",
        "\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "values:\n",
            "---------------------------\n",
            " 0.77| 0.86| 0.98| 0.00|\n",
            "---------------------------\n",
            " 0.67| 0.00|-0.94| 0.00|\n",
            "---------------------------\n",
            " 0.52|-0.36|-0.64|-0.92|\n",
            "policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  U  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-ewQ-sIBE1kG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Q Learning"
      ]
    },
    {
      "metadata": {
        "id": "1CQAr1EIE3xm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 908
        },
        "outputId": "951a93fb-13b1-4658-a661-5fd8e76d663f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530754700271,
          "user_tz": 360,
          "elapsed": 6503,
          "user": {
            "displayName": "Kay Park",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "115858547447612857691"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#from grid_world import standard_grid, negative_grid\n",
        "#from iterative_policy_evaluation import print_values, print_policy\n",
        "#from monte_carlo_es import max_dict\n",
        "#from td0_prediction import random_action\n",
        "\n",
        "GAMMA = 0.9\n",
        "ALPHA = 0.1\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.width):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.height):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "    \n",
        "  \n",
        "def max_dict(d):\n",
        "  # returns the argmax (key) and max (value) from a dictionary\n",
        "  # put this into a function since we are using it so often\n",
        "  max_key = None\n",
        "  max_val = float('-inf')\n",
        "  for k, v in d.items():\n",
        "    if v > max_val:\n",
        "      max_val = v\n",
        "      max_key = k\n",
        "  return max_key, max_val\n",
        "\n",
        "def random_action(a, eps=0.1):\n",
        "  # we'll use epsilon-soft to ensure all states are visited\n",
        "  # what happens if you don't do this? i.e. eps=0\n",
        "  p = np.random.random()\n",
        "  if p < (1 - eps):\n",
        "    return a\n",
        "  else:\n",
        "    return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # NOTE: if we use the standard grid, there's a good chance we will end up with\n",
        "  # suboptimal policies\n",
        "  # e.g.\n",
        "  # ---------------------------\n",
        "  #   R  |   R  |   R  |      |\n",
        "  # ---------------------------\n",
        "  #   R* |      |   U  |      |\n",
        "  # ---------------------------\n",
        "  #   U  |   R  |   U  |   L  |\n",
        "  # since going R at (1,0) (shown with a *) incurs no cost, it's OK to keep doing that.\n",
        "  # we'll either end up staying in the same spot, or back to the start (2,0), at which\n",
        "  # point we whould then just go back up, or at (0,0), at which point we can continue\n",
        "  # on right.\n",
        "  # instead, let's penalize each movement so the agent will find a shorter route.\n",
        "  #\n",
        "  # grid = standard_grid()\n",
        "  grid = negative_grid(step_cost=-0.1)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # no policy initialization, we will derive our policy from most recent Q\n",
        "\n",
        "  # initialize Q(s,a)\n",
        "  Q = {}\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    Q[s] = {}\n",
        "    for a in ALL_POSSIBLE_ACTIONS:\n",
        "      Q[s][a] = 0\n",
        "\n",
        "  # let's also keep track of how many times Q[s] has been updated\n",
        "  update_counts = {}\n",
        "  update_counts_sa = {}\n",
        "  for s in states:\n",
        "    update_counts_sa[s] = {}\n",
        "    for a in ALL_POSSIBLE_ACTIONS:\n",
        "      update_counts_sa[s][a] = 1.0\n",
        "\n",
        "  # repeat until convergence\n",
        "  t = 1.0\n",
        "  deltas = []\n",
        "  for it in range(10000):\n",
        "    if it % 100 == 0:\n",
        "      t += 1e-2\n",
        "    if it % 2000 == 0:\n",
        "      print(\"it:\", it)\n",
        "\n",
        "    # instead of 'generating' an epsiode, we will PLAY\n",
        "    # an episode within this loop\n",
        "    s = (2, 0) # start state\n",
        "    grid.set_state(s)\n",
        "\n",
        "    # the first (s, r) tuple is the state we start in and 0\n",
        "    # (since we don't get a reward) for simply starting the game\n",
        "    # the last (s, r) tuple is the terminal state and the final reward\n",
        "    # the value for the terminal state is by definition 0, so we don't\n",
        "    # care about updating it.\n",
        "    a, _ = max_dict(Q[s])\n",
        "    biggest_change = 0\n",
        "    while not grid.game_over():\n",
        "      a = random_action(a, eps=0.5/t) # epsilon-greedy\n",
        "      # random action also works, but slower since you can bump into walls\n",
        "      # a = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "      r = grid.move(a)\n",
        "      s2 = grid.current_state()\n",
        "\n",
        "      # adaptive learning rate\n",
        "      alpha = ALPHA / update_counts_sa[s][a]\n",
        "      update_counts_sa[s][a] += 0.005\n",
        "\n",
        "      # we will update Q(s,a) AS we experience the episode\n",
        "      old_qsa = Q[s][a]\n",
        "      # the difference between SARSA and Q-Learning is with Q-Learning\n",
        "      # we will use this max[a']{ Q(s',a')} in our update\n",
        "      # even if we do not end up taking this action in the next step\n",
        "      a2, max_q_s2a2 = max_dict(Q[s2])\n",
        "      Q[s][a] = Q[s][a] + alpha*(r + GAMMA*max_q_s2a2 - Q[s][a])\n",
        "      biggest_change = max(biggest_change, np.abs(old_qsa - Q[s][a]))\n",
        "\n",
        "      # we would like to know how often Q(s) has been updated too\n",
        "      update_counts[s] = update_counts.get(s,0) + 1\n",
        "\n",
        "      # next state becomes current state\n",
        "      s = s2\n",
        "     \n",
        "    deltas.append(biggest_change)\n",
        "\n",
        "  plt.plot(deltas)\n",
        "  plt.show()\n",
        "\n",
        "  # determine the policy from Q*\n",
        "  # find V* from Q*\n",
        "  policy = {}\n",
        "  V = {}\n",
        "  for s in grid.actions.keys():\n",
        "    a, max_q = max_dict(Q[s])\n",
        "    policy[s] = a\n",
        "    V[s] = max_q\n",
        "\n",
        "  # what's the proportion of time we spend updating each part of Q?\n",
        "  print(\"update counts:\")\n",
        "  total = np.sum(list(update_counts.values()))\n",
        "  for k, v in update_counts.items():\n",
        "    update_counts[k] = float(v) / total\n",
        "  print_values(update_counts, grid)\n",
        "\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            "-0.10|-0.10|-0.10| 1.00|\n",
            "---------------------------\n",
            "-0.10| 0.00|-0.10|-1.00|\n",
            "---------------------------\n",
            "-0.10|-0.10|-0.10|-0.10|\n",
            "it: 0\n",
            "it: 2000\n",
            "it: 4000\n",
            "it: 6000\n",
            "it: 8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFKCAYAAAAnj5dkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fb810826978>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "update counts:\n",
            "---------------------------\n",
            " 0.26| 0.05| 0.04| 0.00|\n",
            "---------------------------\n",
            " 0.12| 0.00| 0.01| 0.00|\n",
            "---------------------------\n",
            " 0.28| 0.07| 0.05| 0.11|\n",
            "values:\n",
            "---------------------------\n",
            " 0.62| 0.80| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.46| 0.00| 0.80| 0.00|\n",
            "---------------------------\n",
            " 0.31| 0.46| 0.62| 0.46|\n",
            "policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2KyGuq9tIAmh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "while not grid.game_over():\n",
        "      a = random_action(a, eps=0.5/t) # epsilon-greedy\n",
        "      r = grid.move(a)\n",
        "      s2 = grid.current_state()\n",
        "\n",
        "   \n",
        "      alpha = ALPHA / update_counts_sa[s][a]\n",
        "      update_counts_sa[s][a] += 0.005\n",
        "\n",
        "      \n",
        "      old_qsa = Q[s][a]\n",
        "      a2, max_q_s2a2 = max_dict(Q[s2])\n",
        "      Q[s][a] = Q[s][a] + alpha*(r + GAMMA*max_q_s2a2 - Q[s][a])\n",
        "      biggest_change = max(biggest_change, np.abs(old_qsa - Q[s][a]))\n",
        "\n",
        "      update_counts[s] = update_counts.get(s,0) + 1\n",
        "\n",
        "      s = s2\n",
        "     \n",
        "    deltas.append(biggest_change)\n",
        "    \n",
        "    s: is the previous state\n",
        "    \n",
        "a: is the previous action\n",
        "Q(): is the Q-learning algorithm\n",
        "s': is the current state\n",
        "alpha: is the the learning rate, set generally between 0 and 1. Setting it to 0 means that the Q-values are never updated, thereby nothing is learned. Setting alpha to a high value such as 0.9 means that learning can occur quickly.\n",
        "gamma: is the discount factor, also set between 0 and 1. This models the fact that future rewards are worth less than immediate rewards.\n",
        "max,: is the the maximum reward that is attainable in the state following the current one (the reward for taking the optimal action thereafter)."
      ]
    },
    {
      "metadata": {
        "id": "ZYuP-L88FmzT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}